{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bba4ce-e8b2-4187-a762-63e231ed902e",
   "metadata": {},
   "source": [
    "# Original OpenAI CLIP\n",
    "From https://github.com/openai/CLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd4b3b-b814-433d-8378-1b1411fa940e",
   "metadata": {},
   "source": [
    "## Simple usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3945ada7-7ba9-4e8c-8cf9-9afeef96ad5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[0.9927   0.004185 0.002968]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fde283-10b6-475f-ba6e-b84be0ca1981",
   "metadata": {},
   "source": [
    "## Zero shot prediction\n",
    "\n",
    "The code below performs zero-shot prediction using CLIP, as shown in Appendix B in the paper. This example takes an image from the CIFAR-100 dataset, and predicts the most likely labels among the 100 textual labels from the dataset.\n",
    "\n",
    "Note that this example uses the `encode_image()` and `encode_text()` methods that return the encoded features of given inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf7c09c1-bed6-4b11-9478-c8b383f56ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:11<00:00, 15.2MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictions:\n",
      "\n",
      "           snake: 65.53%\n",
      "          turtle: 12.12%\n",
      "    sweet_pepper: 3.87%\n",
      "          lizard: 1.89%\n",
      "       crocodile: 1.72%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Download the dataset\n",
    "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
    "\n",
    "# Prepare the inputs\n",
    "image, class_id = cifar100[3637]\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
    "\n",
    "# Calculate features\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(5)\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83352c4d-775b-4278-9b4a-71ecba43cc19",
   "metadata": {},
   "source": [
    "## Linear-probe evaluation\n",
    "\n",
    "The example below uses scikit-learn to perform logistic regression on image features.\n",
    "\n",
    "Note that the `C` value should be determined via a hyperparameter sweep using a validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9203b847-354c-40e7-86f9-fd9efec66b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:18<00:00,  6.33it/s]\n",
      "100%|██████████| 100/100 [00:15<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        51300     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.60517D+00    |proj g|=  1.53246D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  6.75614D-01    |proj g|=  8.49750D-03\n",
      "\n",
      "At iterate  100    f=  5.84321D-01    |proj g|=  1.01679D-02\n",
      "\n",
      "At iterate  150    f=  5.66909D-01    |proj g|=  3.72494D-03\n",
      "\n",
      "At iterate  200    f=  5.63269D-01    |proj g|=  4.89657D-04\n",
      "\n",
      "At iterate  250    f=  5.62606D-01    |proj g|=  2.58750D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "51300    290    300      1     0     0   8.677D-05   5.625D-01\n",
      "  F =  0.56246845550390634     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "Accuracy = 80.030\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Load the dataset\n",
    "root = os.path.expanduser(\"~/.cache\")\n",
    "train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
    "test = CIFAR100(root, download=True, train=False, transform=preprocess)\n",
    "\n",
    "\n",
    "def get_features(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            features = model.encode_image(images.to(device))\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "# Calculate the image features\n",
    "train_features, train_labels = get_features(train)\n",
    "test_features, test_labels = get_features(test)\n",
    "\n",
    "# Perform logistic regression\n",
    "classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate using the logistic regression classifier\n",
    "predictions = classifier.predict(test_features)\n",
    "accuracy = np.mean((test_labels == predictions).astype(float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7359c07d-3e63-4010-b227-36ee219e191a",
   "metadata": {},
   "source": [
    "# Hugging face CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10bf832c-9ec4-49d5-9b4a-bd6b262bface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0009, 0.0070, 0.6385, 0.3536]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image = Image.open(\"lung_cancer.png\")\n",
    "text = [\"female smoker\", \"male nonsmoker\", \"adenocarcinoma\", \"squamous cell carcinoma\"]\n",
    "\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "logits_per_image = outputs.logits_per_image  # similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # probabilities\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85104630-249d-40dc-892c-9d38759bcefd",
   "metadata": {},
   "source": [
    "# Open source CLIP\n",
    "\n",
    "From https://github.com/mlfoundations/open_clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4874622-0359-4c37-9bee-e6aef31d399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install open_clip_torch\n",
    "# pip install 'open_clip_torch[training]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fab0e574-b6e4-4b42-a205-ce7a1a1b6e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[9.9950e-01, 4.1207e-04, 8.5317e-05]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "model.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0)\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n",
    "\n",
    "with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
