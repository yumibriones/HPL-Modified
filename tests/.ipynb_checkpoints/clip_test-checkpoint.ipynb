{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bba4ce-e8b2-4187-a762-63e231ed902e",
   "metadata": {},
   "source": [
    "# Original OpenAI CLIP\n",
    "From https://github.com/openai/CLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493d665-5ff3-4d39-b7fe-362462a4afde",
   "metadata": {},
   "source": [
    "```\n",
    "conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
    "pip install ftfy regex tqdm\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd4b3b-b814-433d-8378-1b1411fa940e",
   "metadata": {},
   "source": [
    "## Simple usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3945ada7-7ba9-4e8c-8cf9-9afeef96ad5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[0.9927   0.004185 0.002968]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fde283-10b6-475f-ba6e-b84be0ca1981",
   "metadata": {},
   "source": [
    "## Zero shot prediction\n",
    "\n",
    "The code below performs zero-shot prediction using CLIP, as shown in Appendix B in the paper. This example takes an image from the CIFAR-100 dataset, and predicts the most likely labels among the 100 textual labels from the dataset.\n",
    "\n",
    "Note that this example uses the `encode_image()` and `encode_text()` methods that return the encoded features of given inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf7c09c1-bed6-4b11-9478-c8b383f56ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:11<00:00, 15.2MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictions:\n",
      "\n",
      "           snake: 65.53%\n",
      "          turtle: 12.12%\n",
      "    sweet_pepper: 3.87%\n",
      "          lizard: 1.89%\n",
      "       crocodile: 1.72%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Download the dataset\n",
    "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
    "\n",
    "# Prepare the inputs\n",
    "image, class_id = cifar100[3637]\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
    "\n",
    "# Calculate features\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(5)\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83352c4d-775b-4278-9b4a-71ecba43cc19",
   "metadata": {},
   "source": [
    "## Linear-probe evaluation\n",
    "\n",
    "The example below uses scikit-learn to perform logistic regression on image features.\n",
    "\n",
    "Note that the `C` value should be determined via a hyperparameter sweep using a validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9203b847-354c-40e7-86f9-fd9efec66b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:18<00:00,  6.33it/s]\n",
      "100%|██████████| 100/100 [00:15<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        51300     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.60517D+00    |proj g|=  1.53246D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  6.75614D-01    |proj g|=  8.49750D-03\n",
      "\n",
      "At iterate  100    f=  5.84321D-01    |proj g|=  1.01679D-02\n",
      "\n",
      "At iterate  150    f=  5.66909D-01    |proj g|=  3.72494D-03\n",
      "\n",
      "At iterate  200    f=  5.63269D-01    |proj g|=  4.89657D-04\n",
      "\n",
      "At iterate  250    f=  5.62606D-01    |proj g|=  2.58750D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "51300    290    300      1     0     0   8.677D-05   5.625D-01\n",
      "  F =  0.56246845550390634     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "Accuracy = 80.030\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Load the dataset\n",
    "root = os.path.expanduser(\"~/.cache\")\n",
    "train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
    "test = CIFAR100(root, download=True, train=False, transform=preprocess)\n",
    "\n",
    "\n",
    "def get_features(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            features = model.encode_image(images.to(device))\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "# Calculate the image features\n",
    "train_features, train_labels = get_features(train)\n",
    "test_features, test_labels = get_features(test)\n",
    "\n",
    "# Perform logistic regression\n",
    "classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate using the logistic regression classifier\n",
    "predictions = classifier.predict(test_features)\n",
    "accuracy = np.mean((test_labels == predictions).astype(float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7359c07d-3e63-4010-b227-36ee219e191a",
   "metadata": {},
   "source": [
    "# Hugging face CLIP\n",
    "\n",
    "From https://huggingface.co/docs/transformers/model_doc/clip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a71ec59-c9d5-4e8a-96cb-c22d74f731c3",
   "metadata": {},
   "source": [
    "```\n",
    "pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10bf832c-9ec4-49d5-9b4a-bd6b262bface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0009, 0.0070, 0.6385, 0.3536]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image = Image.open(\"lung_cancer.png\")\n",
    "text = [\"female smoker\", \"male nonsmoker\", \"adenocarcinoma\", \"squamous cell carcinoma\"]\n",
    "\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "logits_per_image = outputs.logits_per_image  # similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # probabilities\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85104630-249d-40dc-892c-9d38759bcefd",
   "metadata": {},
   "source": [
    "# Open source CLIP\n",
    "\n",
    "From https://github.com/mlfoundations/open_clip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c7c33-a761-472f-9ad5-a685589560a7",
   "metadata": {},
   "source": [
    "```\n",
    "pip install open_clip_torch\n",
    "pip install 'open_clip_torch[training]'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fab0e574-b6e4-4b42-a205-ce7a1a1b6e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[9.9950e-01, 4.1207e-04, 8.5317e-05]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "model.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0)\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n",
    "\n",
    "with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eceec4-3b56-4fd9-bc66-0a25beb595ad",
   "metadata": {},
   "source": [
    "## Training CLIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cdde0c9-ed4d-4bc5-a4f8-2034aa154f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['img', 'labels', 'patterns', 'slides', 'tiles']\n",
      "[b'TCGA-49-6745-01Z-00-DX7' b'TCGA-68-A59J-01Z-00-DX1'\n",
      " b'TCGA-95-A4VN-01Z-00-DX1' ... b'TCGA-63-5131-01Z-00-DX1'\n",
      " b'TCGA-49-AARE-01Z-00-DX1' b'TCGA-73-7498-01Z-00-DX1']\n",
      "250000\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "file_path = \"/gpfs/home/yb2612/dl4med_25/dl_project/data/scratch_data/hdf5_TCGAFFPE_LUADLUSC_5x_60pc_250K_he_train.h5\"\n",
    "\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    combined_slides_dataset = f['slides'][:]\n",
    "    print(combined_slides_dataset)\n",
    "\n",
    "print(len(combined_slides_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f98f59c-4574-4607-a15c-d51a8658b6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "\n",
    "model, preprocess, tokenizer = open_clip.create_model_and_transforms(\n",
    "    model_name=\"ViT-B-32\",\n",
    "    pretrained=None  # or use \"laion2b_s34b_b79k\" if you want to fine-tune\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bcba8f-46fc-4b8f-b589-8860e4ca6c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class HistopathologyCLIPDataset(Dataset):\n",
    "    def __init__(self, h5_path, caption_csv, transform, tokenizer):\n",
    "        self.h5_file = h5py.File(h5_path, 'r')\n",
    "        self.images = self.h5_file['slides']\n",
    "        self.captions_df = pd.read_csv(caption_csv)\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]  # (H, W, C) or (C, H, W)\n",
    "        if img.shape[-1] == 3:  # assume (H, W, C)\n",
    "            img = Image.fromarray(img.astype(np.uint8))\n",
    "        else:  # (C, H, W)\n",
    "            img = Image.fromarray(np.moveaxis(img, 0, -1).astype(np.uint8))\n",
    "\n",
    "        caption = self.captions_df.iloc[idx]['caption']\n",
    "        img = self.transform(img)\n",
    "        text = self.tokenizer(caption)\n",
    "\n",
    "        return img, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df01e73-4b2a-4250-ba6f-be064a397447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = HistopathologyCLIPDataset(\n",
    "    h5_path=\"/gpfs/home/yb2612/dl4med_25/dl_project/data/scratch_data/hdf5_TCGAFFPE_LUADLUSC_5x_60pc_250K_he_train.h5\",\n",
    "    caption_csv=\"/gpfs/home/yb2612/dl4med_25/dl_project/data/lung_images_captions.csv\",\n",
    "    transform=preprocess,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, texts in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        image_features = model.encode_image(images)\n",
    "        text_features = model.encode_text(texts)\n",
    "\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "        logits_per_image = image_features @ text_features.T\n",
    "        logits_per_text = text_features @ image_features.T\n",
    "\n",
    "        labels = torch.arange(len(images), device=device)\n",
    "        loss = (F.cross_entropy(logits_per_image, labels) + F.cross_entropy(logits_per_text, labels)) / 2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
