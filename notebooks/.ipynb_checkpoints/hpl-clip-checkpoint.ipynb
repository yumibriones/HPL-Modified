{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0cf0fdc-b11a-4af8-9171-0ff77fc00282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Dataset ===\n",
    "class TileCaptionDatasetFromNpy(Dataset):\n",
    "    def __init__(self, embedding_path, filename_path, caption_csv):\n",
    "        # Load data\n",
    "        self.embeddings = np.load(embedding_path, allow_pickle=True)  # shape: [N, embed_dim]\n",
    "        self.filenames = np.load(filename_path, allow_pickle=True)     # shape: [N], dtype='<U...'\n",
    "\n",
    "        # Read caption CSV and create filename â†’ caption mapping\n",
    "        caption_df = pd.read_csv(caption_csv)\n",
    "        caption_df['filepath'] = caption_df['filepath'].str.replace(\"/train_250k/\", \"/train/\", regex=False)\n",
    "        caption_df = caption_df.set_index(\"filepath\")\n",
    "        caption_map = caption_df[\"caption\"].to_dict()\n",
    "\n",
    "        # Build samples: only those with both embedding and caption\n",
    "        self.samples = []\n",
    "        for i, fname in enumerate(self.filenames):\n",
    "            if fname in caption_map:\n",
    "                caption = caption_map[fname]\n",
    "                self.samples.append((self.embeddings[i], caption))\n",
    "\n",
    "        print(f\"[INFO] Matched {len(self.samples)} image-caption pairs out of {len(self.embeddings)}\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tile_embed, caption = self.samples[idx]\n",
    "        tokens = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"tile\": torch.tensor(tile_embed, dtype=torch.float32),\n",
    "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# === ClinicalBERT Text Encoder ===\n",
    "class ClinicalBERTEmbedder(nn.Module):\n",
    "    def __init__(self, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        self.proj = nn.Linear(768, out_dim)  # Project CLS token to same dim as tile embeddings\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token = output.last_hidden_state[:, 0]\n",
    "        return self.proj(cls_token)\n",
    "\n",
    "# === CLIP-style Contrastive Loss ===\n",
    "class CLIPLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(np.log(1 / temperature), dtype=torch.float32))\n",
    "\n",
    "    def forward(self, img_embeds, text_embeds):\n",
    "        img_embeds = F.normalize(img_embeds, dim=-1)\n",
    "        text_embeds = F.normalize(text_embeds, dim=-1)\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "\n",
    "        logits_per_image = img_embeds @ text_embeds.t() * logit_scale\n",
    "        logits_per_text = text_embeds @ img_embeds.t() * logit_scale\n",
    "\n",
    "        targets = torch.arange(img_embeds.size(0), device=img_embeds.device)\n",
    "\n",
    "        loss_i2t = F.cross_entropy(logits_per_image, targets)\n",
    "        loss_t2i = F.cross_entropy(logits_per_text, targets)\n",
    "        return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "# === Training Loop ===\n",
    "def train_one_epoch(dataloader, text_encoder, loss_fn, optimizer, device):\n",
    "    text_encoder.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        tile = batch[\"tile\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        text_embeds = text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(tile, text_embeds)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c445442-bcda-44fb-9531-39e452b0a4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Matched 236635 image-caption pairs out of 551613\n",
      "[Epoch 1/5] Loss: 3.2077\n"
     ]
    }
   ],
   "source": [
    "# === Main ===\n",
    "def main():\n",
    "    # Paths\n",
    "    embedding_path = \"/gpfs/home/yb2612/dl4med_25/dl_project/results/hpl/train/image_embeddings.npy\"\n",
    "    filename_path = \"/gpfs/home/yb2612/dl4med_25/dl_project/results/hpl/train/image_filenames.npy\"\n",
    "    caption_csv = \"/gpfs/home/yb2612/dl4med_25/dl_project/data/scratch_data/hpl-clip/long_consistent_captions/lung_250k_filepath_caption.csv\"\n",
    "\n",
    "    # Hyperparams\n",
    "    batch_size = 64\n",
    "    num_epochs = 5\n",
    "    lr = 2e-5\n",
    "\n",
    "    # Device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Dataset + Dataloader\n",
    "    dataset = TileCaptionDatasetFromNpy(embedding_path, filename_path, caption_csv)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Model & Loss\n",
    "    text_encoder = ClinicalBERTEmbedder().to(device)\n",
    "    loss_fn = CLIPLoss().to(device)\n",
    "    optimizer = torch.optim.AdamW(list(text_encoder.parameters()) + list(loss_fn.parameters()), lr=lr)\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_loss = train_one_epoch(dataloader, text_encoder, loss_fn, optimizer, device)\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {avg_loss:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68050b7e-28e1-4411-8a9e-cef8c98cfe19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
